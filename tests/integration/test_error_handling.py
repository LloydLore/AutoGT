"""
Integration test for error handling and validation per FR-002 requirements.

This test validates robust error handling across all system components:
- Input validation and sanitization
- File format validation and error recovery  
- API error handling and failover mechanisms
- Database constraint validation
- Graceful degradation under failure conditions
- User-friendly error messages and recovery guidance

Error Handling Requirements from FR-002:
- System must validate all inputs and provide clear error messages
- Errors must not expose sensitive system information
- Recovery mechanisms must preserve data integrity
- User guidance must be provided for common error scenarios
"""

import json
import pytest
import tempfile
import shutil
from pathlib import Path
from unittest.mock import patch, Mock

from autogt.cli.main import cli
from click.testing import CliRunner


class TestErrorHandling:
    """Error handling and validation integration test."""
    
    @pytest.fixture
    def runner(self):
        """CLI test runner."""
        return CliRunner()
    
    def test_file_validation_errors(self, runner):
        """Test file format validation and error handling."""
        # Test 1: Invalid CSV format
        invalid_csv_content = """This is not a valid CSV file
Missing headers and proper structure
Should cause validation error"""
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:
            f.write(invalid_csv_content)
            invalid_csv = f.name
        
        result = runner.invoke(cli, [
            'analysis', 'create',
            '--name', 'Invalid CSV Test',
            invalid_csv
        ])
        
        # Should fail with clear error message
        assert result.exit_code != 0
        error_output = result.output.lower()
        
        # Check for appropriate error indicators
        error_keywords = ['invalid', 'format', 'csv', 'header', 'column']
        assert any(keyword in error_output for keyword in error_keywords)
        
        # Should not expose system paths or sensitive information
        assert '/tmp' not in result.output or '/temp' not in result.output
        assert 'traceback' not in error_output
        
        print("✅ Invalid CSV format error handling validated")
        Path(invalid_csv).unlink()
        
        # Test 2: Missing required columns
        missing_columns_csv = """Asset Name,Description
ECU Gateway,Central hub
Missing required columns"""
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:
            f.write(missing_columns_csv)
            missing_csv = f.name
        
        result = runner.invoke(cli, [
            'analysis', 'create',
            '--name', 'Missing Columns Test',
            missing_csv
        ])
        
        assert result.exit_code != 0
        
        # Should specify which columns are missing
        missing_indicators = ['column', 'required', 'missing', 'asset type', 'criticality']
        assert any(indicator in result.output.lower() for indicator in missing_indicators)
        
        print("✅ Missing columns error handling validated")
        Path(missing_csv).unlink()
        
        # Test 3: File size validation (>10MB limit per quickstart.md)
        # Create a very large CSV file
        large_csv_lines = ["Asset Name,Asset Type,Criticality Level,Interfaces,Description"]
        
        # Add many assets to create large file
        for i in range(50000):  # Should create >10MB file
            large_csv_lines.append(
                f"Asset_{i:05d},HARDWARE,HIGH,CAN,Large file test asset {i} with extra description text to increase size"
            )
        
        large_csv_content = "\\n".join(large_csv_lines)
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:
            f.write(large_csv_content)
            large_csv = f.name
        
        # Check if file is actually large
        file_size = Path(large_csv).stat().st_size / (1024 * 1024)  # MB
        
        if file_size > 10:  # Only test if file is actually >10MB
            result = runner.invoke(cli, [
                'analysis', 'create',
                '--name', 'Large File Test',
                large_csv
            ])
            
            # Should either handle gracefully or provide appropriate error
            if result.exit_code != 0:
                size_indicators = ['size', 'large', 'limit', 'mb', 'megabyte']
                assert any(indicator in result.output.lower() for indicator in size_indicators)
                print("✅ Large file size error handling validated")
            else:
                print("✅ Large file processing succeeded (no size limit enforced)")
        \n        Path(large_csv).unlink()\n        \n        # Test 4: Unsupported file extension\n        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:\n            f.write("Asset Name,Asset Type\\nTest,HARDWARE")\n            txt_file = f.name\n        \n        result = runner.invoke(cli, [\n            'analysis', 'create',\n            '--name', 'Unsupported Extension Test',\n            txt_file\n        ])\n        \n        assert result.exit_code != 0\n        extension_indicators = ['extension', 'format', 'unsupported', 'csv', 'excel']\n        assert any(indicator in result.output.lower() for indicator in extension_indicators)\n        \n        print("✅ Unsupported file extension error handling validated")\n        Path(txt_file).unlink()\n    \n    def test_input_validation_errors(self, runner):\n        """Test input validation and sanitization."""\n        # Create valid CSV for testing\n        valid_csv = """Asset Name,Asset Type,Criticality Level,Interfaces,Description\nECU Gateway,HARDWARE,HIGH,CAN,Test asset"""\n        \n        with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:\n            f.write(valid_csv)\n            csv_file = f.name\n        \n        try:\n            # Test 1: Invalid analysis name (empty)\n            result = runner.invoke(cli, [\n                'analysis', 'create',\n                '--name', '',  # Empty name\n                csv_file\n            ])\n            \n            assert result.exit_code != 0\n            name_indicators = ['name', 'empty', 'required', 'invalid']\n            assert any(indicator in result.output.lower() for indicator in name_indicators)\n            \n            # Test 2: Invalid analysis name (too long or special characters)\n            result = runner.invoke(cli, [\n                'analysis', 'create', \n                '--name', 'A' * 300,  # Very long name\n                csv_file\n            ])\n            \n            # Should handle long names gracefully\n            if result.exit_code != 0:\n                length_indicators = ['length', 'long', 'limit', 'character']\n                assert any(indicator in result.output.lower() for indicator in length_indicators)\n            \n            # Test 3: Invalid UUID format (for commands that expect analysis ID)\n            result = runner.invoke(cli, [\n                'analysis', 'show',\n                'invalid-uuid-format'\n            ])\n            \n            assert result.exit_code != 0\n            uuid_indicators = ['uuid', 'id', 'invalid', 'format', 'analysis']\n            assert any(indicator in result.output.lower() for indicator in uuid_indicators)\n            \n            # Test 4: Non-existent analysis ID (valid UUID format)\n            result = runner.invoke(cli, [\n                'analysis', 'show',\n                '550e8400-e29b-41d4-a716-446655440000'  # Valid UUID format\n            ])\n            \n            assert result.exit_code != 0\n            not_found_indicators = ['not found', 'does not exist', 'unknown', 'missing']\n            assert any(indicator in result.output.lower() for indicator in not_found_indicators)\n            \n            print("✅ Input validation error handling validated")\n        \n        finally:\n            Path(csv_file).unlink()\n    \n    def test_workflow_step_validation_errors(self, runner):\n        """Test workflow step prerequisite validation."""\n        # Create valid analysis\n        valid_csv = """Asset Name,Asset Type,Criticality Level,Interfaces,Description\nECU Gateway,HARDWARE,HIGH,CAN,Test asset"""\n        \n        with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:\n            f.write(valid_csv)\n            csv_file = f.name\n        \n        try:\n            result = runner.invoke(cli, [\n                'analysis', 'create',\n                '--name', 'Step Validation Test',\n                csv_file\n            ])\n            \n            assert result.exit_code == 0\n            analysis_data = json.loads(result.output)\n            analysis_id = analysis_data['analysis_id']\n            \n            # Test 1: Skip step 1 (assets) and try step 3 (threats)\n            result = runner.invoke(cli, [\n                'threats', 'identify', analysis_id\n            ])\n            \n            assert result.exit_code != 0\n            prereq_indicators = ['prerequisite', 'step', 'complete', 'first', 'asset']\n            assert any(indicator in result.output.lower() for indicator in prereq_indicators)\n            \n            # Test 2: Try to export incomplete analysis\n            with tempfile.NamedTemporaryFile(suffix='.json', delete=False) as f:\n                output_file = f.name\n            \n            result = runner.invoke(cli, [\n                'export', '--format', 'json',\n                '--output', output_file,\n                analysis_id\n            ])\n            \n            # Should handle incomplete analysis gracefully\n            if result.exit_code != 0:\n                incomplete_indicators = ['incomplete', 'step', 'complete', 'analysis']\n                assert any(indicator in result.output.lower() for indicator in incomplete_indicators)\n            \n            Path(output_file).unlink()\n            \n            # Test 3: Complete assets step, then try step 6 (risks) without prerequisites\n            result = runner.invoke(cli, ['assets', 'define', analysis_id])\n            \n            # Now try to calculate risks without completing intermediate steps\n            result = runner.invoke(cli, [\n                'risks', 'calculate', analysis_id\n            ])\n            \n            assert result.exit_code != 0\n            risk_prereq_indicators = ['threat', 'impact', 'prerequisite', 'step']\n            assert any(indicator in result.output.lower() for indicator in risk_prereq_indicators)\n            \n            print("✅ Workflow step validation error handling validated")\n        \n        finally:\n            Path(csv_file).unlink()\n    \n    def test_api_error_handling(self, runner):\n        """Test API error handling and failover mechanisms."""\n        # Create analysis for API testing\n        valid_csv = """Asset Name,Asset Type,Criticality Level,Interfaces,Description\nECU Gateway,HARDWARE,HIGH,CAN,Test asset"""\n        \n        with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:\n            f.write(valid_csv)\n            csv_file = f.name\n        \n        try:\n            result = runner.invoke(cli, [\n                'analysis', 'create',\n                '--name', 'API Error Test',\n                csv_file\n            ])\n            \n            assert result.exit_code == 0\n            analysis_data = json.loads(result.output)\n            analysis_id = analysis_data['analysis_id']\n            \n            # Complete assets step\n            runner.invoke(cli, ['assets', 'define', analysis_id])\n            \n            # Test 1: Gemini API connection failure\n            with patch('autogt.core.ai.gemini_client.GeminiClient._make_request') as mock_request:\n                mock_request.side_effect = Exception("Connection timeout")\n                \n                result = runner.invoke(cli, [\n                    'threats', 'identify', '--auto-generate', analysis_id\n                ])\n                \n                # Should handle API failure gracefully\n                if result.exit_code != 0:\n                    api_error_indicators = ['api', 'connection', 'timeout', 'service', 'unavailable']\n                    assert any(indicator in result.output.lower() for indicator in api_error_indicators)\n                    \n                    # Should not expose detailed technical errors\n                    assert 'traceback' not in result.output.lower()\n                    assert 'exception' not in result.output.lower()\n            \n            # Test 2: Gemini API rate limiting\n            with patch('autogt.core.ai.gemini_client.GeminiClient._make_request') as mock_request:\n                mock_request.side_effect = Exception("Rate limit exceeded")\n                \n                result = runner.invoke(cli, [\n                    'threats', 'identify', '--auto-generate', analysis_id\n                ])\n                \n                if result.exit_code != 0:\n                    rate_limit_indicators = ['rate', 'limit', 'quota', 'exceeded', 'retry']\n                    assert any(indicator in result.output.lower() for indicator in rate_limit_indicators)\n            \n            # Test 3: Malformed API response\n            with patch('autogt.core.ai.gemini_client.GeminiClient._make_request') as mock_request:\n                mock_request.return_value = {\n                    'candidates': [{\n                        'content': {\n                            'parts': [{\n                                'text': 'Invalid JSON response that cannot be parsed as threat data'\n                            }]\n                        }\n                    }]\n                }\n                \n                result = runner.invoke(cli, [\n                    'threats', 'identify', '--auto-generate', analysis_id\n                ])\n                \n                if result.exit_code != 0:\n                    parse_error_indicators = ['format', 'parse', 'invalid', 'response', 'data']\n                    assert any(indicator in result.output.lower() for indicator in parse_error_indicators)\n            \n            print("✅ API error handling validated")\n        \n        finally:\n            Path(csv_file).unlink()\n    \n    def test_database_constraint_validation(self, runner):\n        """Test database constraint validation and error handling."""\n        # Create analysis for database testing\n        valid_csv = """Asset Name,Asset Type,Criticality Level,Interfaces,Description\nECU Gateway,HARDWARE,HIGH,CAN,Test asset"""\n        \n        with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:\n            f.write(valid_csv)\n            csv_file = f.name\n        \n        try:\n            # Test 1: Duplicate analysis creation\n            analysis_name = "Duplicate Test Analysis"\n            \n            # Create first analysis\n            result1 = runner.invoke(cli, [\n                'analysis', 'create',\n                '--name', analysis_name,\n                csv_file\n            ])\n            \n            # Try to create duplicate (same name)\n            result2 = runner.invoke(cli, [\n                'analysis', 'create', \n                '--name', analysis_name,\n                csv_file\n            ])\n            \n            # Should either succeed (allowing duplicates) or fail with clear message\n            if result2.exit_code != 0:\n                duplicate_indicators = ['duplicate', 'exists', 'name', 'already']\n                assert any(indicator in result2.output.lower() for indicator in duplicate_indicators)\n            \n            # Test 2: Invalid data type constraints\n            if result1.exit_code == 0:\n                analysis_data = json.loads(result1.output)\n                analysis_id = analysis_data['analysis_id']\n                \n                # Complete assets to test constraint validation\n                runner.invoke(cli, ['assets', 'define', analysis_id])\n                \n                # Try operations with invalid data that should be caught by constraints\n                # This depends on the specific implementation of data validation\n            \n            print("✅ Database constraint validation completed")\n        \n        finally:\n            Path(csv_file).unlink()\n    \n    def test_file_system_error_handling(self, runner):\n        """Test file system error handling and recovery."""\n        # Test 1: Missing file\n        result = runner.invoke(cli, [\n            'analysis', 'create',\n            '--name', 'Missing File Test',\n            '/nonexistent/path/file.csv'\n        ])\n        \n        assert result.exit_code != 0\n        file_not_found_indicators = ['not found', 'no such file', 'does not exist', 'missing']\n        assert any(indicator in result.output.lower() for indicator in file_not_found_indicators)\n        \n        # Test 2: Permission denied\n        # Create file and remove read permissions (Unix-like systems)\n        valid_csv = """Asset Name,Asset Type,Criticality Level,Interfaces,Description\nECU Gateway,HARDWARE,HIGH,CAN,Test asset"""\n        \n        with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:\n            f.write(valid_csv)\n            restricted_file = f.name\n        \n        try:\n            # Remove read permissions\n            import stat\n            Path(restricted_file).chmod(stat.S_IWUSR)  # Write-only\n            \n            result = runner.invoke(cli, [\n                'analysis', 'create',\n                '--name', 'Permission Test',\n                restricted_file\n            ])\n            \n            # Should handle permission error gracefully\n            if result.exit_code != 0:\n                permission_indicators = ['permission', 'access', 'denied', 'read']\n                error_msg = result.output.lower()\n                \n                # Should either have permission error or succeed (depending on system)\n                if any(indicator in error_msg for indicator in permission_indicators):\n                    print("✅ Permission denied error handling validated")\n                else:\n                    print("✅ File access succeeded despite permission changes")\n        \n        finally:\n            # Restore permissions and cleanup\n            try:\n                Path(restricted_file).chmod(stat.S_IRUSR | stat.S_IWUSR)\n                Path(restricted_file).unlink()\n            except:\n                pass\n        \n        # Test 3: Disk space / write errors during export\n        valid_csv = """Asset Name,Asset Type,Criticality Level,Interfaces,Description\nECU Gateway,HARDWARE,HIGH,CAN,Test asset"""\n        \n        with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:\n            f.write(valid_csv)\n            csv_file = f.name\n        \n        try:\n            result = runner.invoke(cli, [\n                'analysis', 'create',\n                '--name', 'Export Error Test',\n                csv_file\n            ])\n            \n            if result.exit_code == 0:\n                analysis_data = json.loads(result.output)\n                analysis_id = analysis_data['analysis_id']\n                \n                # Complete some steps\n                runner.invoke(cli, ['assets', 'define', analysis_id])\n                \n                # Try to export to invalid directory\n                result = runner.invoke(cli, [\n                    'export', '--format', 'json',\n                    '--output', '/invalid/directory/output.json',\n                    analysis_id\n                ])\n                \n                assert result.exit_code != 0\n                write_error_indicators = ['directory', 'path', 'write', 'create', 'invalid']\n                assert any(indicator in result.output.lower() for indicator in write_error_indicators)\n        \n        finally:\n            Path(csv_file).unlink()\n        \n        print("✅ File system error handling validated")\n    \n    def test_graceful_degradation(self, runner):\n        """Test graceful degradation under failure conditions."""\n        # Create analysis for degradation testing\n        valid_csv = """Asset Name,Asset Type,Criticality Level,Interfaces,Description\nECU Gateway,HARDWARE,HIGH,CAN,Test asset\nInfotainment,SOFTWARE,MEDIUM,USB,Test asset 2"""\n        \n        with tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False) as f:\n            f.write(valid_csv)\n            csv_file = f.name\n        \n        try:\n            result = runner.invoke(cli, [\n                'analysis', 'create',\n                '--name', 'Graceful Degradation Test',\n                csv_file\n            ])\n            \n            assert result.exit_code == 0\n            analysis_data = json.loads(result.output)\n            analysis_id = analysis_data['analysis_id']\n            \n            # Complete assets step\n            result = runner.invoke(cli, ['assets', 'define', analysis_id])\n            assert result.exit_code == 0\n            \n            # Test 1: AI service unavailable - should continue with manual mode\n            with patch('autogt.core.ai.gemini_client.GeminiClient._make_request') as mock_request:\n                mock_request.side_effect = Exception("Service unavailable")\n                \n                # Try threat identification with fallback enabled\n                result = runner.invoke(cli, [\n                    'threats', 'identify',\n                    '--fallback-manual',  # Should fall back to manual mode\n                    analysis_id\n                ])\n                \n                # Should either succeed with fallback or provide clear guidance\n                if result.exit_code != 0:\n                    fallback_indicators = ['fallback', 'manual', 'offline', 'alternative']\n                    assert any(indicator in result.output.lower() for indicator in fallback_indicators)\n            \n            # Test 2: Partial data export (some components fail)\n            # Complete some steps\n            runner.invoke(cli, ['impact', 'rate', analysis_id])\n            \n            # Export should work with partial data\n            with tempfile.NamedTemporaryFile(suffix='.json', delete=False) as f:\n                output_file = f.name\n            \n            result = runner.invoke(cli, [\n                'export', '--format', 'json',\n                '--output', output_file,\n                '--partial-ok',  # Allow partial export\n                analysis_id\n            ])\n            \n            # Should succeed or provide clear partial data indication\n            if result.exit_code == 0:\n                # Check that export contains available data\n                if Path(output_file).exists():\n                    with open(output_file, 'r') as f:\n                        export_data = json.load(f)\n                    \n                    # Should contain at least asset data\n                    assert 'asset_inventory' in export_data or 'assets' in str(export_data)\n            \n            Path(output_file).unlink()\n            \n            print("✅ Graceful degradation validated")\n        \n        finally:\n            Path(csv_file).unlink()\n    \n    def test_user_friendly_error_messages(self, runner):\n        """Test that error messages are user-friendly and actionable."""\n        # Test various error scenarios and validate message quality\n        \n        error_scenarios = [\n            {\n                'command': ['analysis', 'create', '--name', 'Test', '/nonexistent.csv'],\n                'expected_guidance': ['check', 'path', 'exists', 'file'],\n                'description': 'Missing file guidance'\n            },\n            {\n                'command': ['analysis', 'show', 'invalid-id'],\n                'expected_guidance': ['uuid', 'format', 'analysis', 'id'],\n                'description': 'Invalid ID format guidance'\n            },\n            {\n                'command': ['export', '--format', 'invalid', '--output', 'test.txt', 'test-id'],\n                'expected_guidance': ['format', 'supported', 'json', 'excel'],\n                'description': 'Invalid format guidance'\n            }\n        ]\n        \n        for scenario in error_scenarios:\n            result = runner.invoke(cli, scenario['command'])\n            \n            assert result.exit_code != 0\n            \n            error_msg = result.output.lower()\n            \n            # Check for user-friendly guidance\n            has_guidance = any(guide in error_msg for guide in scenario['expected_guidance'])\n            \n            # Error messages should be helpful\n            assert has_guidance, f"Missing guidance for {scenario['description']}: {result.output}"\n            \n            # Should not contain technical internals\n            technical_terms = ['traceback', 'exception', 'stack trace', 'debug']\n            has_technical = any(term in error_msg for term in technical_terms)\n            \n            assert not has_technical, f"Error message too technical for {scenario['description']}: {result.output}"\n        \n        print("✅ User-friendly error messages validated")